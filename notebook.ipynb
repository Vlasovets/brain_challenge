{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 1000 Brains Study Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='logo.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 94830 columns are the unique brain connections, i.e. the pairwise correlation between the activation in different brain regions.\n",
    "There are 436 brain regions, therefore there are 436*435/2 unique connections. The way the number are created is that first a full correlation matrix (436 x 436) is computed and then the lower triangular matrix is converted into a vector.\n",
    " \n",
    "The last two columns are age and sex.\n",
    " \n",
    "When the file is loaded with `pandas.read_csv` the first 94830 columns are labelled only by number and, while the last two colums are laballed as “age” and “sex_f0_m1”.\n",
    " \n",
    "So the first number is the connection between region 2 and 1, the second one connection between 3 and 1, then 3  and 2, … and so on.\n",
    " \n",
    "The rows are simply the different subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "df_train = pd.read_csv('train.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
       "       ...\n",
       "       '94822', '94823', '94824', '94825', '94826', '94827', '94828', '94829',\n",
       "       'age', 'sex_f0_m1'],\n",
       "      dtype='object', length=94832)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_cv(x,y,svc):\n",
    "    predsum = 0\n",
    "    # cross validation\n",
    "    cv = KFold(n_splits=10)\n",
    "    for train, test in cv.split(X=x):\n",
    "        svc.fit(x[train], y[train])\n",
    "        pred = svc.predict(x[test])\n",
    "        prediction = (pred == y[test]).sum() / float(len(y[test]))\n",
    "        predsum = predsum + prediction\n",
    "        mean_acc = predsum / 10\n",
    "    return mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = ''\n",
    "save_dir = '.'\n",
    "\n",
    "X_train = pd.read_csv(f'{data_path}train.csv') \n",
    "y_train = X_train[['sex_f0_m1']]\n",
    "\n",
    "X_train_feat = X_train.drop(columns=['sex_f0_m1','age'])\n",
    "\n",
    "n_samples_train = X_train_feat.shape[0]\n",
    "n_features_train = X_train_feat.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(751, 94830)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'C': 10.0, 'gamma': 0.0001} with a score of 0.80\n",
      "Within sample prediction 1.000\n"
     ]
    }
   ],
   "source": [
    "n_components = min(n_samples_train, n_features_train)\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(X_train_feat)\n",
    "X_train_red = pca.transform(X_train_feat)\n",
    "\n",
    "y_train = np.ravel(y_train)\n",
    "\n",
    "C_range = np.logspace(-5, 10, 16)\n",
    "gamma_range = np.logspace(-10, 5, 16)\n",
    "\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "cv = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\n",
    "grid.fit(X_train_red, y_train)\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (grid.best_params_, grid.best_score_))\n",
    "\n",
    "svc = grid.best_estimator_\n",
    "joblib.dump(svc, f'{save_dir}/model_1000brains')\n",
    "\n",
    "#acc = comp_cv(x_train,y_train,svc)\n",
    "pred_within = svc.predict(X_train_red)\n",
    "acc_within = (pred_within == y_train).sum() / float(len(y_train))\n",
    "print(\"Within sample prediction %0.3f\" % (acc_within))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(f'{data_path}submission_valid.csv') \n",
    "X_test_feat = X_test.drop(columns=['sex_f0_m1','age'])\n",
    "X_test_red = pca.transform(X_test_feat)\n",
    "\n",
    "y_pred = svc.predict(X_test_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(y_pred, columns=['sex_f0_m1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can open the submision.csv file (File -> Open) file and download it! After you download it, you can upload the `submission.csv` containing only the predictions not the features, to the challenge frontend. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
